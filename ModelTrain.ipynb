{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:43.902996Z",
     "start_time": "2025-03-06T01:49:39.596049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BartTokenizer, \\\n",
    "    BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re"
   ],
   "id": "bd767e2754870729",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 08:49:42.692385: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 08:49:42.701177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741225782.712184   34091 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741225782.715364   34091 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 08:49:42.726781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load your train and validation datasets",
   "id": "db8c2bace9056d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.061622Z",
     "start_time": "2025-03-06T01:49:43.907155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your train and validation datasets\n",
    "num_row = 2000\n",
    "\n",
    "train_df = pd.read_csv(\"resources/train.csv\", nrows=num_row)\n",
    "val_df = pd.read_csv(\"resources/validation.csv\", nrows=num_row)"
   ],
   "id": "7d69456627eb29c6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.374338Z",
     "start_time": "2025-03-06T01:49:44.368842Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_df.head())",
   "id": "cafb052387a2edcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing:",
   "id": "bfa5fe13faa51b99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.552011Z",
     "start_time": "2025-03-06T01:49:44.492390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['article'] = train_df['article'].str.lower()\n",
    "val_df['article'] = val_df['article'].str.lower()\n",
    "\n",
    "train_texts = train_df['article'].tolist()\n",
    "train_labels = train_df['highlights'].tolist()\n",
    "\n",
    "val_texts = val_df['article'].tolist()\n",
    "val_labels = val_df['highlights'].tolist()"
   ],
   "id": "333edf3d7ae0847e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.580933Z",
     "start_time": "2025-03-06T01:49:44.578018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Sample highlight:\", train_labels[0])  # Should not be 0 or empty\n",
    "print(\"Sample article:\", train_texts[0]) "
   ],
   "id": "344e688cc65b8bb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample highlight: Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
      "Sample article: by . associated press . published: . 14:11 est, 25 october 2013 . | . updated: . 15:36 est, 25 october 2013 . the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it's important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.697630Z",
     "start_time": "2025-03-06T01:49:44.692632Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Train Labels Sample:\", train_labels[:5])\n",
   "id": "36a033e6c1b90aea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels Sample: ['Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .', 'Criminal complaint: Cop used his role to help cocaine traffickers .\\nRalph Mata, an internal affairs lieutenant, allegedly helped group get guns .\\nHe also arranged to pay two assassins in a murder plot, a complaint alleges .', \"Craig Eccleston-Todd, 27, had drunk at least three pints before driving car .\\nWas using phone when he veered across road in Yarmouth, Isle of Wight .\\nCrashed head-on into 28-year-old Rachel Titley's car, who died in hospital .\\nPolice say he would have been over legal drink-drive limit at time of crash .\\nHe was found guilty at Portsmouth Crown Court of causing death by dangerous driving .\", \"Nina dos Santos says Europe must be ready to accept sanctions will hurt both sides .\\nTargeting Russia's business community would be one way of sapping their support for President Putin, she says .\\nBut she says Europe would have a hard time keeping its factories going without power from the east .\", 'Fleetwood top of League One after 2-0 win at Scunthorpe .\\nPeterborough, Bristol City, Chesterfield and Crawley all drop first points of the season .\\nStand-in striker Matt Done scores a hat-trick as Rochdale thrash Crewe 5-2 .\\nWins for Notts County and Yeovil .\\nCoventry/Bradford and Oldham/Port Vale both end in draws .\\nA late Stephen Bywater own goal denies Gillingham three points against Millwall .']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:44.761181Z",
     "start_time": "2025-03-06T01:49:44.757707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the target summaries are in train_labels\n",
    "print(f\"Train label for first article: {train_labels[0]}\")  # Should show the summary, not 0\n"
   ],
   "id": "3204daa98aa068a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label for first article: Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "49ad80584ed1979e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:47.764084Z",
     "start_time": "2025-03-06T01:49:44.814530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(\"cuda\")"
   ],
   "id": "7b51688999c461c2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:49:47.831453Z",
     "start_time": "2025-03-06T01:49:47.788308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Find the average length of sequences\n",
    "avg_length = np.mean([len(text.split()) for text in train_texts])\n",
    "print(f\"Average sequence length: {avg_length}\")\n"
   ],
   "id": "2f884a68fa86532a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 694.314\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a Dataset class to handle the data more easily",
   "id": "45cdc11f0bbb0ffe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:53:58.602827Z",
     "start_time": "2025-03-06T01:53:58.596036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Ensure text and label are valid strings\n",
    "        if not isinstance(text, str) or not isinstance(label, str):\n",
    "            text = \"\"\n",
    "            label = \"\"\n",
    "\n",
    "        # Tokenize the article (input text)\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenize the summary (target text)\n",
    "        label_encoding = self.tokenizer(label, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': label_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n"
   ],
   "id": "2e40d83486e93877",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare DataLoader",
   "id": "26d1d7b84894b759"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:53:59.405015Z",
     "start_time": "2025-03-06T01:53:59.401073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SummarizationDataset(train_texts, train_labels, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "val_dataset = SummarizationDataset(val_texts, val_labels, tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=2, pin_memory=True)"
   ],
   "id": "5244a542bac52e13",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load a Pretrained BERT Model",
   "id": "642c076531c9546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:53:59.946226Z",
     "start_time": "2025-03-06T01:53:59.941965Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)",
   "id": "22443d02f6e77761",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training loop",
   "id": "caa2590aea75e3d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:54:00.878806Z",
     "start_time": "2025-03-06T01:54:00.875590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import torch\n",
    "\n",
    "def calculate_bleu(predictions, references, smoothing_function=None):\n",
    "    predictions = [pred.split() for pred in predictions]  # Convert predictions to list of words\n",
    "    references = [[ref.split()] for ref in references]  # Convert references to list of lists of words\n",
    "    return corpus_bleu(references, predictions, smoothing_function=smoothing_function)\n",
    "\n"
   ],
   "id": "b41291a87671f80f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:07:51.953914Z",
     "start_time": "2025-03-06T01:54:01.895612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "train_steps = len(train_dataloader) * epochs\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=train_steps)\n",
    "\n",
    "gradient_accumulation_steps = 2  # Accumulate gradients over 4 steps\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move to GPU\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Mixed precision training: use autocast for forward and loss calculation\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Scale loss and accumulate gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Accumulate gradients and update optimizer every gradient_accumulation_steps\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()  # Update scheduler after every optimizer step\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(\"cuda\", non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "                val_loss += outputs.loss.item()\n",
    "    \n",
    "            # Generate model predictions\n",
    "            summary_ids = model.generate(batch['input_ids'], num_beams=2, max_length=100, early_stopping=True)\n",
    "            summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "            val_preds.extend(summaries)\n",
    "    \n",
    "            val_labels_list.extend(tokenizer.batch_decode(batch['labels'], skip_special_tokens=True))\n",
    "    \n",
    "    # Compute BLEU Score\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "    bleu_score = calculate_bleu(val_preds, val_labels_list, smoothing_function)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} Train Loss: {avg_train_loss:.4f} BLEU Score: {bleu_score:.4f}\")\n"
   ],
   "id": "b836617559af9dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Train Loss: 4.3996 BLEU Score: 0.0773\n",
      "Epoch 2/3 Train Loss: 3.5398 BLEU Score: 0.0954\n",
      "Epoch 3/3 Train Loss: 3.1271 BLEU Score: 0.0970\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving model",
   "id": "c5c0c80c381d5797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:08:00.092664Z",
     "start_time": "2025-03-06T02:07:57.515110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"summarization_model\")\n",
    "tokenizer.save_pretrained(\"summarization_model\")\n",
    "print(\"Model saved to bart_model.pth\")"
   ],
   "id": "5924dade1185d601",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bart_model.pth\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to generate a summary from the trained model",
   "id": "77e80ac344fb209d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:08:00.170985Z",
     "start_time": "2025-03-06T02:08:00.166677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summary(text, model, tokenizer, max_length=150):\n",
    "    # Preprocess and tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Generate the summary using the trained model\n",
    "    summary_ids = model.generate(inputs['input_ids'].to(\"cuda\"), num_beams=4, max_length=max_length, early_stopping=True)\n",
    "    \n",
    "    # Print the summary IDs to understand the output\n",
    "    print(f\"Generated Token IDs: {summary_ids}\")\n",
    "\n",
    "    # Decode the summary and return it\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ],
   "id": "bd8557cb061eca66",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:08:00.953317Z",
     "start_time": "2025-03-06T02:08:00.223704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_summary = generate_summary(train_texts[0], model, tokenizer)\n",
    "print(f\"Generated Summary: '{generated_summary}'\")  # Notice the quotes to identify if it's just whitespace\n"
   ],
   "id": "3dbd1529101144be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token IDs: tensor([[    2,     0,   387, 44517,   610, 41303,   102,    34,    57,  6443,\n",
      "            19, 24426,    83,  6793,   479, 50118,   894,    34,    57,  4924,\n",
      "             7, 24426,    83,  7910,    11,    39,   184,    11,  1261,   479,\n",
      "             2]], device='cuda:0')\n",
      "Generated Summary: 'Bishop John Folda has been diagnosed with hepatitis A virus .\n",
      "He has been exposed to hepatitis A infection in his home in Florida .'\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:08:03.511110Z",
     "start_time": "2025-03-06T02:08:03.506833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Sample article:\", train_texts[0])\n",
    "print(\"Sample highlight:\", train_labels[0])\n"
   ],
   "id": "2ac037143b6e8c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample article: by . associated press . published: . 14:11 est, 25 october 2013 . | . updated: . 15:36 est, 25 october 2013 . the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it's important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\n",
      "Sample highlight: Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
